import pandas as pd
import re
import vertexai
from vertexai.generative_models import GenerativeModel
import os
import time
import json
class TranscriptProcessor:
    def __init__(self, transcript_text, target_language, 
                 excel_path="../dataset/Training/Knowledge Dataset.xlsx",
                 project_id="hackathon-450900", location="us-central1", dir="results"):
        """
        åˆå§‹åŒ–ï¼š
          - transcript_textï¼šé€å­—ç¨¿çš„æ–‡å­—å…§å®¹ï¼ˆè‹¥åŸä¾†æœ‰æª”æ¡ˆè·¯å¾‘ï¼Œå¯å…ˆè‡ªè¡Œè®€æª”ï¼‰
          - target_languageï¼šç›®æ¨™èªè¨€ (ä¾‹å¦‚ "cmn-Hant-TW"ã€"en-US"ã€"ja-JP"ã€"de-DE")
          - excel_pathï¼šproper noun è³‡æ–™è¡¨çš„è·¯å¾‘
          - project_id èˆ‡ locationï¼šVertex AI åˆå§‹åŒ–æ‰€éœ€åƒæ•¸
        """
        self.transcript_text = transcript_text
        self.target_language = target_language
        self.excel_path = excel_path
        self.project_id = project_id
        self.location = location
        # dir = "results"
        os.makedirs(dir, exist_ok=True)
        self.dir = dir
        # åˆå§‹åŒ– Vertex AI ç’°å¢ƒ
        vertexai.init(project=self.project_id, location=self.location)
        # è®€å– Excel ä¸¦å»ºç«‹ proper_nouns_dict
        self.proper_nouns_dict = self.load_proper_nouns()
        # å®šç¾©è¼¸å‡ºé€å­—ç¨¿æª”æ¡ˆåç¨±
        self.output_transcript_path = f"transcript_{self.target_language}.txt"

    def load_proper_nouns(self):
        """
        è®€å– Excel ä¸¦å»ºç«‹ `proper_nouns_dict` å’Œ `all_proper_nouns_dict`
        - `proper_nouns_dict` ä»¥ `en-US` ç‚º keyï¼Œå­˜å…¥æ‰€æœ‰èªè¨€å°æ‡‰ç‰ˆæœ¬çš„ Proper Noun å’Œæè¿°ã€‚
        - `all_proper_nouns_dict` å…è¨±å¾ **ä»»ä½•èªè¨€ç‰ˆæœ¬çš„ Proper Noun æ‰¾åˆ° `en-US` key**ï¼Œç¢ºä¿æ‰€æœ‰èªè¨€éƒ½èƒ½åŒ¹é…ã€‚
        """
        sheets = ["cmn-Hant-TW", "en-US", "ja-JP", "de-DE"]
        
        # è®€å– Excelï¼Œæ¯å€‹ sheet ç§»é™¤æ¬„ä½å‰å¾Œç©ºç™½
        dfs = {
            sheet: pd.read_excel(self.excel_path, sheet_name=sheet).rename(columns=lambda x: x.strip())
            for sheet in sheets
        }

        proper_nouns_dict = {}
        all_proper_nouns_dict = {}

        for idx in dfs["en-US"].index:
            en_proper_noun = dfs["en-US"].loc[idx, "Proper Noun"]
            
            if pd.notna(en_proper_noun):
                proper_nouns = {}
                descriptions = {}

                for sheet in sheets:
                    pn = dfs[sheet].loc[idx, "Proper Noun"]
                    desc = dfs[sheet].loc[idx, "Description"]

                    if pd.notna(pn):
                        proper_nouns[sheet] = pn.strip()  # Proper Noun
                        all_proper_nouns_dict[pn.strip()] = en_proper_noun  # åå‘æ˜ å°„åˆ° `en-US`

                    descriptions[sheet] = desc.strip() if pd.notna(desc) else "N/A"  # Description

                if proper_nouns:
                    proper_nouns_dict[en_proper_noun] = {
                        "Type": dfs["en-US"].loc[idx, "Type"],
                        "Proper Nouns": proper_nouns,
                        "Descriptions": descriptions
                    }

        self.all_proper_nouns_dict = all_proper_nouns_dict  # å­˜ç‚ºé¡åˆ¥è®Šæ•¸ï¼Œæ–¹ä¾¿æŸ¥æ‰¾
        return proper_nouns_dict



    def improved_replace_proper_nouns(self, transcript, target_language):
        """
        æ”¹é€²æ–¹æ³•ï¼ˆé€è¡Œåµæ¸¬ proper nounï¼‰ï¼š
        1. å¾ proper_nouns_dict ä¸­è®€å–æ‰€æœ‰ proper nounï¼ˆå„èªè¨€ç‰ˆæœ¬ï¼‰ã€‚
        2. é‡å°æ¯å€‹é …ç›®ï¼Œä¾æ“šæ‰€æœ‰è®Šé«”å»ºç«‹ä¸€çµ„å‘½åç¾¤çµ„ï¼ˆnamed groupï¼‰ï¼Œä½¿ç”¨è‡ªè¨‚é‚Šç•Œè™•ç†è‹±æ–‡åŠä¸­æ–‡æ··é›œæƒ…æ³ã€‚
        3. é€è¡Œä½¿ç”¨ re.sub æ­é…æ›¿æ›å‡½å¼é€²è¡Œæ›¿æ›ï¼ŒåŒæ™‚è¨˜éŒ„å‡ºç¾çš„ proper nounï¼ˆä¾åŸå§‹é †åºï¼‰ã€‚
        4. å›å‚³æ›¿æ›å¾Œçš„é€å­—ç¨¿ã€ä¾é †åºçš„ proper noun æ¸…å–® èˆ‡è™•ç†æ™‚é–“ã€‚
        """
        start_time = time.time()
        
        # å»ºç«‹å‘½åç¾¤çµ„èˆ‡å°æ‡‰ proper noun è³‡è¨Š
        group_patterns = []
        group_mapping = {}
        i = 0
        for en_term, metadata in self.proper_nouns_dict.items():
            target_proper = metadata["Proper Nouns"].get(target_language) or metadata["Proper Nouns"].get("en-US")
            if not target_proper:
                continue
            
            variants = sorted(set(metadata["Proper Nouns"].values()), key=lambda x: len(x), reverse=True)
            variant_patterns = []
            for v in variants:
                v_clean = v.strip()
                is_ascii = all(ord(ch) < 128 for ch in v_clean)
                contains_space = " " in v_clean
                if is_ascii and not contains_space:
                    left_boundary = r'(?:(?<=^)|(?<=[^A-Za-z0-9]))'
                    right_boundary = r'(?=$|[^A-Za-z0-9])'
                    variant_patterns.append(left_boundary + re.escape(v_clean) + right_boundary)
                else:
                    variant_patterns.append(re.escape(v_clean).replace(r'\\ ', r'\\s+'))
            combined_entry_pattern = "(" + "|".join(variant_patterns) + ")"
            group_name = f"pn_{i}"
            i += 1
            group_patterns.append(f"(?P<{group_name}>" + combined_entry_pattern + ")")
            group_mapping[group_name] = {
                "Proper Noun": target_proper,
                "Original Variants": variants,
                "Type": metadata.get("Type", ""),
                "Description": metadata["Descriptions"].get(target_language, ""),
                "Count": 0
            }
        
        if not group_patterns:
            execution_time = round(time.time() - start_time, 2)
            return transcript, [], execution_time
        
        combined_pattern = "|".join(group_patterns)
        regex = re.compile(combined_pattern, re.IGNORECASE)
        
        ordered_detections = []
        
        def replacement_func(match):
            for group_name, value in match.groupdict().items():
                if value is not None:
                    group_mapping[group_name]["Count"] += 1
                    ordered_detections.append(group_mapping[group_name]["Proper Noun"])
                    return "{" + group_mapping[group_name]["Proper Noun"] + "}"
            return match.group(0)
        
        updated_lines = []
        for line in transcript.splitlines():
            new_line = regex.sub(replacement_func, line)
            updated_lines.append(new_line)
        updated_transcript = "\n".join(updated_lines)
        execution_time = round(time.time() - start_time, 2)
        # print(f'order_detections: {ordered_detections}')
        return updated_transcript, ordered_detections, execution_time


    def translate_with_vertex_ai(self, text, target_language):
        """
        ä½¿ç”¨ Gemini æ¨¡å‹é€²è¡Œç¿»è­¯ï¼š
          - å°‡æ‰€æœ‰ä¸åœ¨å¤§æ‹¬è™Ÿ {} ä¸­çš„éƒ¨åˆ†ç¿»è­¯æˆç›®æ¨™èªè¨€ï¼Œ
          - ä¿ç•™ {} ä¸­çš„å…§å®¹ä¸è®Šï¼Œ
          - ç¿»è­¯çµæœä¸­ä¸åŒ…å«å¤§æ‹¬è™Ÿï¼ˆå¾ŒçºŒæœƒç§»é™¤ï¼‰ã€‚
        """
        language_prompt_map = {
            "cmn-Hant-TW": "ç¹é«”ä¸­æ–‡",
            "en-US": "è‹±æ–‡",
            "ja-JP": "æ—¥æ–‡",
            "de-DE": "å¾·æ–‡"
        }
        lang = language_prompt_map.get(target_language, "ç¹é«”ä¸­æ–‡")

        prompt = (
            f"è«‹å°‡ä¸‹åˆ—æ–‡æœ¬å…¨éƒ¨ç¿»è­¯æˆ{lang}ï¼Œ"
            "è«‹æ³¨æ„ï¼šæ–‡æœ¬ä¸­ç”¨å¤§æ‹¬è™Ÿ {} åŒ…å«çš„éƒ¨åˆ†å¿…é ˆä¿æŒåŸæ¨£ï¼Œ"
            "å…¶é¤˜éƒ¨åˆ†è«‹å®Œæ•´ç¿»è­¯ä¸”ä¸è¦ä¿ç•™ä»»ä½•åŸæ–‡ï¼Œ"
            "æ–‡æœ¬ç‚ºï¼š\n"
            # "ç¿»è­¯çµæœä¸­è«‹ä¸è¦åŒ…å«å¤§æ‹¬è™Ÿï¼Œè«‹åƒ…å›å‚³ç¿»è­¯å¾Œçš„æ–‡æœ¬"
            f"{text}"
        )
        try:
            model = GenerativeModel("gemini-1.5-pro-002")
            response = model.generate_content(prompt)
        except Exception as e:
            print(f"Error: {e}")
            print("Switching to an older model version...")
            model = GenerativeModel("gemini-1.5-pro-001")
            response = model.generate_content(prompt)
        
        return response.text
    def detect_proper_nouns_with_prompt(self, transcript):
        start_time = time.time()
        proper_nouns_list = []
        for metadata in self.proper_nouns_dict.values():
            for pn in metadata["Proper Nouns"].values():
                if pn:
                    proper_nouns_list.append(pn.strip())

        proper_nouns_list = sorted(set(proper_nouns_list), key=lambda x: len(x), reverse=True)

        prompt = (
            f"ä»¥ä¸‹æ˜¯ Proper Noun æ¸…å–®ï¼š{', '.join(proper_nouns_list)}ã€‚\n"
            "è«‹åˆ†æä¸‹é¢çš„é€å­—ç¨¿ï¼Œæ‰¾å‡ºæ¸…å–®ä¸­æ‰€æœ‰å‡ºç¾éçš„ Proper Nounï¼Œä¸¦ç›´æ¥åœ¨åŸå§‹æ–‡æœ¬å…§æ›¿æ›æ‰å®ƒå€‘ã€‚\n"
            "è«‹ç¢ºä¿ Proper Noun ä½¿ç”¨æ¸…å–®ä¸­çš„æ¨™æº–å¯«æ³•ï¼Œè‹¥å­˜åœ¨è®Šé«”ï¼ˆå¦‚å¤§å°å¯«ã€ç¸®å¯«ï¼‰ï¼Œè«‹çµ±ä¸€æ›¿æ›ç‚ºæ¸…å–®ä¸­æœ€æ¨™æº–çš„åç¨±ã€‚\n"
            "è«‹ç¢ºä¿ Proper Noun **é‡è¤‡å‡ºç¾æ™‚** ä¹Ÿéƒ½åŒ…å«åœ¨ `proper_nouns` åˆ—è¡¨ä¸­ï¼Œä¸¦ä¸”æŒ‰ç…§å®ƒå€‘åœ¨é€å­—ç¨¿ä¸­ **å¯¦éš›å‡ºç¾çš„é †åº** è¨˜éŒ„ã€‚\n"
            "è«‹å‹™å¿…è¼¸å‡º **Python dictionary æ ¼å¼çš„ JSON å­—ç¬¦ä¸²**ï¼ˆä¸å«ä»»ä½•è§£é‡‹æ€§æ–‡å­—ï¼‰ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š\n\n"
            "```json\n"
            "{\n"
            '  "transcript": "é€™è£¡å¡«å…¥æ›¿æ›å¾Œçš„é€å­—ç¨¿",\n'
            '  "proper_nouns": ["Proper Noun 1", "Proper Noun 2", ...]\n'
            "}\n"
            "```\n\n"
            "è«‹ç›´æ¥è¼¸å‡º JSON å­—ç¬¦ä¸²ï¼Œä¸è¦åŠ å…¥ä»»ä½•å…¶ä»–èªªæ˜ã€‚\n"
            f"é€å­—ç¨¿å…§å®¹ï¼š\n{transcript}"
        )

        try:
            model = GenerativeModel("gemini-1.5-pro-002")
            response = model.generate_content(prompt)
        except Exception as e:
            print(f"Error: {e}")
            print("Switching to an older model version...")
            model = GenerativeModel("gemini-1.5-pro-001")
            response = model.generate_content(prompt)

        cleaned_response = response.text.strip()
        execution_time = round(time.time() - start_time, 2)
        # print(f'cleaned_response: {cleaned_response}')
        try:
            if cleaned_response.startswith("```json"):
                cleaned_response = cleaned_response.strip("```json").strip("```").strip()

            response_json = json.loads(cleaned_response)
            cleaned_transcript = response_json.get("transcript", transcript)
            mapped_detection_list = response_json.get("proper_nouns", [])

            if not isinstance(mapped_detection_list, list):
                mapped_detection_list = []

            # **é€²è¡Œ Proper Noun æ›¿æ›**
            updated_mapped_list = []
            for proper in mapped_detection_list:
                target_proper = self.get_target_proper_noun(proper, self.target_language)
                updated_mapped_list.append(target_proper)

                # é€éæ­£å‰‡è¡¨é”å¼æ›¿æ›æ‰€æœ‰ proper noun ç‚º {ç›®æ¨™èªè¨€ç‰ˆæœ¬}
                pattern = re.escape(proper).replace(r'\ ', r'\\s+')
                cleaned_transcript = re.sub(pattern, f"{{{target_proper}}}", cleaned_transcript, flags=re.IGNORECASE)

        except json.JSONDecodeError as e:
            print(f"JSON è§£æéŒ¯èª¤ï¼Œå›é€€åˆ°åŸå§‹å…§å®¹: {e}")
            cleaned_transcript = transcript
            updated_mapped_list = []

        detection_result = {
            "proper_nouns": updated_mapped_list,
            "time": f"{execution_time} s"
        }
        # print(f"detection_result: {detection_result} ")

        return cleaned_transcript, detection_result

    def get_target_proper_noun(self, candidate_str, target_language):
        for en_term, metadata in self.proper_nouns_dict.items():
            if candidate_str in metadata["Proper Nouns"].values():
                return metadata["Proper Nouns"].get(target_language, candidate_str)
        return candidate_str


    def replace_gemini_proper_nouns(self, transcript, gemini_proper_list, target_language):
        new_transcript = transcript
        for proper in gemini_proper_list:
            candidate_str = proper if isinstance(proper, str) else proper.get("proper_noun", "")
            target_proper = self.get_target_proper_noun(candidate_str, target_language)
            if target_proper:
                pattern = re.escape(candidate_str).replace(r'\ ', r'\\s+')
                new_transcript = re.sub(pattern, "{" + target_proper + "}", new_transcript, flags=re.IGNORECASE)
        return new_transcript
    def process(self):
        """
        å®Œæ•´è™•ç†é€å­—ç¨¿æµç¨‹ï¼š
        1. å…ˆé€²è¡Œ Regular Expression æ›¿æ›ä¸¦è¨˜éŒ„ Proper Noun åŠè™•ç†æ™‚é–“ã€‚
        2. ä½¿ç”¨ Gemini AI åµæ¸¬ Proper Noun ä¸¦è¨˜éŒ„çµæœåŠè™•ç†æ™‚é–“ã€‚
        3. å‘¼å« Gemini æ¨¡å‹ç¿»è­¯ï¼ˆç¿»è­¯æ™‚ä¿ç•™ {} ä¸­å…§å®¹ä¸è®Šï¼‰ã€‚
        4. ç§»é™¤ç¿»è­¯çµæœä¸­å¯èƒ½æ®˜ç•™çš„ {}ã€‚
        5. å„²å­˜ç¿»è­¯å¾Œçš„é€å­—ç¨¿ã€proper noun è¨ˆæ•¸æª”åŠæè¿°æª”ã€‚
        6. é¡¯ç¤º Regular Expression å’Œ Gemini AI åµæ¸¬çš„ Proper Noun çµæœã€‚
        """
        replaced_transcript, proper_noun_list_regex, execution_time = self.improved_replace_proper_nouns(self.transcript_text, self.target_language)
        
        # **å„²å­˜ Regex åµæ¸¬çš„ Proper Noun**
        regex_filename = f"./{self.dir}/term_{self.target_language}.txt"
        with open(regex_filename, "w", encoding="utf-8") as f:
            f.write(f"Regex Proper Noun åµæ¸¬çµæœ (è™•ç†æ™‚é–“: {execution_time} ç§’)\n")
            for i, proper_noun in enumerate(proper_noun_list_regex, start=1):
                f.write(f"{i}. {proper_noun}\n")

        # **åŸ·è¡Œ Gemini Proper Noun åµæ¸¬**
        llm_replaced_transcript, detection_result = self.detect_proper_nouns_with_prompt(self.transcript_text)

        # **å°‡ Gemini åµæ¸¬çš„ Proper Noun ä¹Ÿç¶“é improved_replace_proper_nouns è™•ç†**
        llm_replaced_transcript, testproper_noun_list_regex, testexecution_time = self.improved_replace_proper_nouns(llm_replaced_transcript, self.target_language)

        # **ç¿»è­¯æœ€çµ‚çµæœ**
        translated_transcript = self.translate_with_vertex_ai(llm_replaced_transcript, self.target_language)

        # **å„²å­˜ Gemini åµæ¸¬çš„ Proper Noun**
        gemini_detection_filename = f"./{self.dir}/gemini_detection_{self.target_language}.txt"
        with open(gemini_detection_filename, "w", encoding="utf-8") as f:
            f.write(f"Gemini prompt Proper Noun Detection (time: {detection_result['time']})\n")
            f.write("=" * 50 + "\n")
            for i, proper_noun in enumerate(detection_result["proper_nouns"], start=1):
                f.write(f"{i}. {proper_noun}\n")
            f.write("-" * 50 + "\n")

        # **å„²å­˜ç¿»è­¯å¾Œçš„é€å­—ç¨¿**
        final_transcript = translated_transcript.replace("{", "").replace("}", "")
        with open(f"./{self.dir}/{self.output_transcript_path}", "w", encoding="utf-8") as file:
            file.write(final_transcript)

        # **ğŸ”¹ å„²å­˜ Proper Noun æè¿°æª”**
        desc_filename = f"./{self.dir}/description_{self.target_language}.txt"
        
        proper_noun_desc = {}

        for noun in detection_result["proper_nouns"]:
            # ğŸ”¹ æ‰¾åˆ°å°æ‡‰çš„ `en-US` Key
            matched_key = self.all_proper_nouns_dict.get(noun, noun)
            if matched_key:
                # print(f"Matched Key: {matched_key} -> {noun}")
            # ğŸ”¹ å–å¾—ç›®æ¨™èªè¨€çš„ Proper Noun å’Œæè¿°
                proper_noun_target = self.proper_nouns_dict.get(matched_key, {}).get("Proper Nouns", {}).get(self.target_language, noun)
                description = self.proper_nouns_dict.get(matched_key, {}).get("Descriptions", {}).get(self.target_language, "N/A")

                proper_noun_desc[proper_noun_target] = description

        # ğŸ”¹ å„²å­˜æè¿°æª”
        desc_filename = f"./{self.dir}/description_{self.target_language}.txt"
        with open(desc_filename, "w", encoding="utf-8") as f:
            for noun, description in proper_noun_desc.items():
                f.write(f"{noun}: {description}\n")

if __name__ == "__main__":
    # ç›´æ¥æŒ‡å®šè¼¸å…¥çš„ txt æª”æ¡ˆè·¯å¾‘

    transcript_file = "Testing_twoStage.txt"
    folder_name = os.path.splitext(transcript_file)[0]
    with open(transcript_file, "r", encoding="utf-8") as f:
        transcript_text = f.read()
    # æŒ‡å®š target_languageï¼Œä¾‹å¦‚ "cmn-Hant-TW" æˆ– "en-US"
    target_language = "cmn-Hant-TW"

    # å»ºç«‹ TranscriptProcessor å¯¦ä¾‹ (è«‹ç¢ºèª Excel è·¯å¾‘æ­£ç¢º)
    processor = TranscriptProcessor(
        transcript_text=transcript_text,
        target_language=target_language,
        excel_path="../dataset/Training/Knowledge Dataset.xlsx",
        project_id="hackathon-450900",
        location="us-central1",
        dir=folder_name
    )

    processor.process()
