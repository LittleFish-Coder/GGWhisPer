import pandas as pd
import re
import vertexai
from vertexai.generative_models import GenerativeModel
import os

class TranscriptProcessor:
    def __init__(self, transcript_text, target_language, 
                 excel_path="../dataset/Training/Knowledge Dataset.xlsx",
                 project_id="hackathon-450410", location="us-central1"):
        """
        åˆå§‹åŒ–ï¼š
          - transcript_textï¼šé€å­—ç¨¿çš„æ–‡å­—å…§å®¹ï¼ˆè‹¥åŸä¾†æœ‰æª”æ¡ˆè·¯å¾‘ï¼Œå¯å…ˆè‡ªè¡Œè®€æª”ï¼‰
          - target_languageï¼šç›®æ¨™èªè¨€ (ä¾‹å¦‚ "cmn-Hant-TW"ã€"en-US"ã€"ja-JP"ã€"de-DE")
          - excel_pathï¼šproper noun è³‡æ–™è¡¨çš„è·¯å¾‘
          - project_id èˆ‡ locationï¼šVertex AI åˆå§‹åŒ–æ‰€éœ€åƒæ•¸
        """
        self.transcript_text = transcript_text
        self.target_language = target_language
        self.excel_path = excel_path
        self.project_id = project_id
        self.location = location
        os.makedirs("results", exist_ok=True)

        # åˆå§‹åŒ– Vertex AI ç’°å¢ƒ
        vertexai.init(project=self.project_id, location=self.location)
        # è®€å– Excel ä¸¦å»ºç«‹ proper_nouns_dict
        self.proper_nouns_dict = self.load_proper_nouns()
        # å®šç¾©è¼¸å‡ºé€å­—ç¨¿æª”æ¡ˆåç¨±
        self.output_transcript_path = f"transcript_{self.target_language}.txt"

    def load_proper_nouns(self):
        """
        è®€å– Excel ä¸¦å»ºç«‹ proper_nouns_dict
        """
        sheets = ["cmn-Hant-TW", "en-US", "ja-JP", "de-DE"]
        # è®€å–å„å€‹ sheetï¼Œä¸¦ç§»é™¤æ¬„ä½å‰å¾Œç©ºç™½
        dfs = {sheet: pd.read_excel(self.excel_path, sheet_name=sheet).rename(columns=lambda x: x.strip())
               for sheet in sheets}

        proper_nouns_dict = {}
        for idx in dfs["en-US"].index:
            en_proper_noun = dfs["en-US"].loc[idx, "Proper Noun"]
            if pd.notna(en_proper_noun):
                proper_nouns = {}
                for sheet in sheets:
                    pn = dfs[sheet].loc[idx, "Proper Noun"]
                    if pd.notna(pn):
                        proper_nouns[sheet] = pn.strip()    # type: ignore
                if proper_nouns:
                    proper_nouns_dict[en_proper_noun] = {
                        "Type": dfs["en-US"].loc[idx, "Type"],
                        "Proper Nouns": proper_nouns,
                        "Descriptions": {lang: dfs[lang].loc[idx, "Description"] for lang in sheets}
                    }
        return proper_nouns_dict

    

    def improved_replace_proper_nouns(self, transcript, target_language):
        """
        æ”¹é€²æ–¹æ³•ï¼š
        1. å…è¨± **æ‰€æœ‰èªè¨€** éƒ½é€²è¡Œ **å¤§å°å¯«ä¸æ•æ„Ÿ** åŒ¹é… (re.IGNORECASE)ã€‚
        2. å–®è© Proper Noun ä½¿ç”¨ `\b` ç¢ºä¿ç¨ç«‹åŒ¹é…ï¼Œå…è¨±æ¨™é»ç¬¦è™Ÿè®Šé«” (e.g., `BigQuery.`)ã€‚
        3. å¤šè© Proper Noun å…è¨±è®Šé«”ç©ºç™½åŒ¹é… (e.g., `CloudFunction`ã€`Cloud  Function`)ã€‚
        """
        replacement_log = []
        updated_transcript = transcript  # é€æ­¥æ›´æ–°æ–‡æœ¬

        # ğŸ”¹ **å®Œå…¨å»é™¤ `en-US` å’Œ `de-DE` çš„å¤§å°å¯«æ•æ„Ÿè¨­å®š** ğŸ”¹
        case_sensitive = False  # æ‰€æœ‰èªè¨€éƒ½è¨­ç‚ºä¸å€åˆ†å¤§å°å¯«

        for en_term, metadata in self.proper_nouns_dict.items():
            target_proper = metadata["Proper Nouns"].get(target_language)
            if not target_proper:
                continue

            # æ”¶é›†æ‰€æœ‰èªè¨€ç‰ˆæœ¬çš„ Proper Nounï¼Œä¾é•·åº¦ç”±é•·åˆ°çŸ­æ’åº
            variants = sorted(set(metadata["Proper Nouns"].values()), key=lambda x: len(x), reverse=True)
            pattern_parts = []
            
            for v in variants:
                v_clean = v.strip()
                is_ascii = all(ord(ch) < 128 for ch in v_clean)  # åˆ¤æ–·æ˜¯å¦ç‚ºç´” ASCII
                contains_space = " " in v_clean  # åˆ¤æ–·æ˜¯å¦ç‚ºå¤šè©

                if is_ascii and not contains_space:
                    # **å–®è© Proper Noun (å¦‚ BigQuery) -> ä½¿ç”¨ `\b` ä½†å…è¨±æ¨™é»ç¬¦è™Ÿè®Šé«”**
                    pattern_parts.append(r'\b' + re.escape(v_clean) + r'[\b,.!?]?')
                else:
                    # **å¤šè© Proper Noun (å¦‚ Cloud Function) -> å…è¨±è®Šé«”ç©ºæ ¼**
                    pattern_parts.append(re.escape(v_clean).replace(r'\ ', r'\s*'))

            # çµ„åˆæ­£å‰‡è¡¨é”å¼
            combined_pattern = "|".join(pattern_parts)

            # **ä½¿ç”¨å¤§å°å¯«ä¸æ•æ„ŸåŒ¹é… (`re.IGNORECASE`)**
            pattern_regex = re.compile(combined_pattern, re.IGNORECASE)

            # é€²è¡ŒåŒ¹é…
            matches = pattern_regex.findall(updated_transcript)
            count = len(matches)
            if count > 0:
                # **å°‡æ‰€æœ‰åŒ¹é…åˆ°çš„ Proper Noun ç”¨ {ç›®æ¨™ proper noun} æ›¿æ›**
                updated_transcript = pattern_regex.sub("{" + target_proper + "}", updated_transcript)
                log_entry = {
                    "Proper Noun": target_proper,
                    "Original Variants": variants,
                    "Type": metadata.get("Type", ""),
                    "Description": metadata["Descriptions"].get(target_language, ""),
                    "Count": count
                }
                replacement_log.append(log_entry)

        return updated_transcript, replacement_log


    def translate_with_vertex_ai(self, text, target_language):
        """
        ä½¿ç”¨ Gemini æ¨¡å‹é€²è¡Œç¿»è­¯ï¼š
          - å°‡æ‰€æœ‰ä¸åœ¨å¤§æ‹¬è™Ÿ {} ä¸­çš„éƒ¨åˆ†ç¿»è­¯æˆç›®æ¨™èªè¨€ï¼Œ
          - ä¿ç•™ {} ä¸­çš„å…§å®¹ä¸è®Šï¼Œ
          - ç¿»è­¯çµæœä¸­ä¸åŒ…å«å¤§æ‹¬è™Ÿï¼ˆå¾ŒçºŒæœƒç§»é™¤ï¼‰ã€‚
        """
        language_prompt_map = {
            "cmn-Hant-TW": "ç¹é«”ä¸­æ–‡",
            "en-US": "è‹±æ–‡",
            "ja-JP": "æ—¥æ–‡",
            "de-DE": "å¾·æ–‡"
        }
        lang = language_prompt_map.get(target_language, "ç¹é«”ä¸­æ–‡")

        prompt = (
            f"è«‹å°‡ä¸‹åˆ—æ–‡æœ¬å…¨éƒ¨ç¿»è­¯æˆ{lang}ï¼Œ"
            "è«‹æ³¨æ„ï¼šæ–‡æœ¬ä¸­ç”¨å¤§æ‹¬è™Ÿ {} åŒ…å«çš„éƒ¨åˆ†å¿…é ˆä¿æŒåŸæ¨£ï¼Œ"
            "å…¶é¤˜éƒ¨åˆ†è«‹å®Œæ•´ç¿»è­¯ä¸”ä¸è¦ä¿ç•™ä»»ä½•åŸæ–‡ï¼Œ"
            "ç¿»è­¯çµæœä¸­è«‹ä¸è¦åŒ…å«å¤§æ‹¬è™Ÿï¼Œè«‹åƒ…å›å‚³ç¿»è­¯å¾Œçš„æ–‡æœ¬ï¼š\n"
            f"{text}"
        )
        try:
            model = GenerativeModel("gemini-1.5-pro-002")
            response = model.generate_content(prompt)
        except Exception as e:
            print(f"Error: {e}")
            print("Switching to an older model version...")
            model = GenerativeModel("gemini-1.5-pro-001")
            response = model.generate_content(prompt)
        
        return response.text

    def process(self):
        """
        å®Œæ•´è™•ç†é€å­—ç¨¿æµç¨‹ï¼š
          1. å…ˆé€²è¡Œ proper noun æ›¿æ›ï¼ˆä¸¦ä»¥ {} åŒ…è£¹ï¼‰ã€‚
          2. å‘¼å« Gemini æ¨¡å‹ç¿»è­¯ï¼ˆç¿»è­¯æ™‚ä¿ç•™ {} ä¸­å…§å®¹ä¸è®Šï¼‰ã€‚
          3. ç§»é™¤ç¿»è­¯çµæœä¸­å¯èƒ½æ®˜ç•™çš„ {}ã€‚
          4. åˆ†åˆ¥å„²å­˜ç¿»è­¯å¾Œçš„é€å­—ç¨¿ã€proper noun è¨ˆæ•¸æª”åŠæè¿°æª”ã€‚
        """
        # (1) Proper noun æ›¿æ›
        replaced_transcript, replacement_log = self.improved_replace_proper_nouns(self.transcript_text, self.target_language)
        # (2) å‘¼å« Gemini ç¿»è­¯ï¼ˆä¿ç•™ {} ä¸­å…§å®¹ä¸è®Šï¼‰
        translated_transcript = self.translate_with_vertex_ai(replaced_transcript, self.target_language)
        # (3) ç§»é™¤ç¿»è­¯çµæœä¸­å¯èƒ½æ®˜ç•™çš„å¤§æ‹¬è™Ÿ
        final_transcript = translated_transcript.replace("{", "").replace("}", "")

        # å„²å­˜ç¿»è­¯å¾Œçš„é€å­—ç¨¿
        with open(f"./results/{self.output_transcript_path}", "w", encoding="utf-8") as file:
            file.write(final_transcript)

        # (a) å„²å­˜ proper noun è¨ˆæ•¸æª”ï¼šæ¯å€‹ proper noun ä¾å‡ºç¾æ¬¡æ•¸å¯«å…¥å¤šè¡Œ
        noun_filename = f"./results/term_{self.target_language}.txt"
        with open(noun_filename, "w", encoding="utf-8") as f:
            for entry in replacement_log:
                for _ in range(entry["Count"]):
                    f.write(entry["Proper Noun"] + "\n")

        # (b) å„²å­˜ proper noun æè¿°æª”ï¼šåŒä¸€å€‹ proper noun åªä¿ç•™ä¸€æ¬¡
        desc_filename = f"./results/description_{self.target_language}.txt"
        proper_noun_desc = {}
        for entry in replacement_log:
            noun = entry["Proper Noun"]
            if noun not in proper_noun_desc:
                proper_noun_desc[noun] = entry["Description"]
        with open(desc_filename, "w", encoding="utf-8") as f:
            for noun in sorted(proper_noun_desc.keys()):
                f.write(f"{noun}: {proper_noun_desc[noun]}\n")

        print(f"âœ… Proper Noun æ›¿æ› & ç¿»è­¯å®Œæˆï¼çµæœå„²å­˜è‡³: {self.output_transcript_path}")
        print(f"âœ… åµæ¸¬ proper noun æª”æ¡ˆç”¢ç”Ÿå®Œæˆï¼š{noun_filename} èˆ‡ {desc_filename}")
    def detect_proper_nouns_with_prompt(self, transcript):
        """
        å°‡ Excel ä¸­æ‰€æœ‰ proper nounï¼ˆå››å€‹èªè¨€ç‰ˆæœ¬ï¼‰æ”¾å…¥ promptï¼Œ
        è«‹æ¨¡å‹åµæ¸¬é€å­—ç¨¿ä¸­å‡ºç¾çš„ proper noun åŠå…¶æ¬¡æ•¸ï¼Œ
        ä¸¦è¨ˆç®—è™•ç†æ‰€èŠ±è²»çš„æ™‚é–“ï¼Œ
        æœ€å¾Œç›´æ¥å›å‚³ JSON æ ¼å¼çš„å­—ä¸²ï¼Œä¾‹å¦‚ï¼š
        {
        "DDR Ratio": "3",
        "DP": "3",
        "Nachtschicht": "1",
        "EC": "3",
        "Cloud Function": "1",
        "BigQuery": "1",
        "time": "5.66 s"
        }
        """
        import time
        import json

        # é–‹å§‹è¨ˆæ™‚
        start_time = time.time()

        # å¾ proper_nouns_dict ä¸­æ“·å–æ‰€æœ‰èªè¨€ç‰ˆæœ¬çš„ proper noun åˆ—è¡¨
        proper_nouns_list = []
        for metadata in self.proper_nouns_dict.values():
            for lang, pn in metadata["Proper Nouns"].items():
                if pn:
                    proper_nouns_list.append(pn.strip())
        # å»é™¤é‡è¤‡ä¸¦çµ„åˆæˆä¸€å€‹å­—ä¸²
        proper_nouns_list = list(set(proper_nouns_list))
        proper_nouns_str = ", ".join(sorted(proper_nouns_list, key=lambda x: len(x), reverse=True))
        # print(f"proper_nouns_str: {proper_nouns_str}")  

        # å»ºæ§‹ promptï¼šå‘ŠçŸ¥æ¨¡å‹åªéœ€æª¢æŸ¥æ¸…å–®ä¸­çš„ proper noun
        prompt = (
            f"ä»¥ä¸‹æ˜¯ proper noun æ¸…å–®ï¼š{proper_nouns_str}\n"
            "è«‹åˆ†æä¸‹é¢çš„é€å­—ç¨¿ï¼Œæ‰¾å‡ºæ¸…å–®ä¸­æ‰€æœ‰å‡ºç¾éçš„ proper nounï¼Œä¸¦è¨ˆç®—å®ƒå€‘å‡ºç¾çš„æ¬¡æ•¸ã€‚\n"
            "è«‹åªè€ƒæ…®ä¸Šè¿°æ¸…å–®ä¸­çš„è©ï¼Œä¸¦ä»¥ JSON æ ¼å¼è¼¸å‡ºï¼Œä¾‹å¦‚ï¼š\n"
            '[{"proper_noun": "ProperNoun", "count": occurrence_count}, ...]\n'
            f"é€å­—ç¨¿å…§å®¹ï¼š\n{transcript}"
        )
        
        try:
            model = GenerativeModel("gemini-1.5-pro-002")
            response = model.generate_content(prompt)
        except Exception as e:
            print(f"Error: {e}")
            print("Switching to an older model version...")
            model = GenerativeModel("gemini-1.5-pro-001")
            response = model.generate_content(prompt)
        
        raw_output = response.text
        # çµæŸè¨ˆæ™‚
        end_time = time.time()
        execution_time = end_time - start_time

        # è™•ç† raw_outputï¼šç§»é™¤å¯èƒ½å­˜åœ¨çš„ code block æ ¼å¼
        cleaned_result = raw_output.strip()
        if cleaned_result.startswith("```json"):
            cleaned_result = "\n".join(cleaned_result.splitlines()[1:-1])
        try:
            detection_list = json.loads(cleaned_result)
            # å°‡åˆ—è¡¨è½‰æ›ç‚ºå­—å…¸ï¼škey ç‚º proper_nounï¼Œvalue ç‚º count
            detection_dict = {entry["proper_noun"]: entry["count"] for entry in detection_list}
        except Exception as e:
            print("ç„¡æ³•è§£æ detection_result ç‚º JSONï¼Œä½¿ç”¨ç©ºçµæœã€‚", e)
            detection_dict = {}
        
        # å°‡è™•ç†æ™‚é–“åŠ å…¥çµæœä¸­ï¼ˆå››æ¨äº”å…¥åˆ° 2 ä½å°æ•¸ï¼‰
        detection_dict["time"] = f"{round(execution_time, 2)} s"
        
        # ç›´æ¥å›å‚³æ ¼å¼åŒ–çš„ JSON å­—ä¸²
        return json.dumps(detection_dict, ensure_ascii=False, indent=2)

if __name__ == "__main__":
    # ç›´æ¥æŒ‡å®šè¼¸å…¥çš„ txt æª”æ¡ˆè·¯å¾‘
    transcript_file = "transcript.txt"
    
    with open(transcript_file, "r", encoding="utf-8") as f:
        transcript_text = f.read()

    # æŒ‡å®š target_languageï¼Œä¾‹å¦‚ "cmn-Hant-TW" æˆ– "en-US"
    target_language = "en-US"

    # å»ºç«‹ TranscriptProcessor å¯¦ä¾‹ (è«‹ç¢ºèª Excel è·¯å¾‘æ­£ç¢º)
    processor = TranscriptProcessor(
        transcript_text=transcript_text,
        target_language=target_language,
        excel_path="../dataset/Training/Knowledge Dataset.xlsx",
        project_id="hackathon-450410",
        location="us-central1"
    )

    # å‘¼å« detect_proper_nouns_with_prompt ä¸¦ç›´æ¥å°å‡º JSON æ ¼å¼çš„è¼¸å‡ºçµæœ
    result_json = processor.detect_proper_nouns_with_prompt(transcript_text)
    print(result_json)
